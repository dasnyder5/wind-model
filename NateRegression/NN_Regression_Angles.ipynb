{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "# Requirements:\n",
    "# (1) pytorch \n",
    "# (2) numpy \n",
    "# (3) matplotlib\n",
    "# (4) pandas\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from ttictoc import tic, toc\n",
    "import csv\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define speeds (m/s) corresponding to 0-40 Hz settings in wind tunnel\n",
    "### (Just for record-keeping)\n",
    "hzArray = np.array((0, 5, 10, 15, 20, 25, 30, 35, 40))\n",
    "speedArray = np.array((0.00, 1.26363735, 1.58562983, 2.07066356, 2.571993, 3.18291372, 3.75322345, 4.33626595, 4.91413509))\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossWireDataset(Dataset):\n",
    "    '''\n",
    "    Dataset class for pytorch-based learning tailored to crosswire model training. This method \n",
    "    essentially is feature learning of a specific, reduced set of features from the sensor readings, \n",
    "    namely: \n",
    "\n",
    "    [Input Features]\n",
    "       --- The maximal (absolute) voltage reading (voltage)\n",
    "       --- The index of the maximal (absolute) voltage reading (integer, {1-6})\n",
    "       --- The (regularized) ratio of the adjacent sensors (voltage/voltage)\n",
    "    [Predictions]\n",
    "       --- The gust speed (m/s)\n",
    "       --- The gust incident angle (radians)\n",
    "\n",
    "    '''\n",
    "    def __init__(self, magFile, angFile, readingsFile, transform=None, target_transform=None):\n",
    "        # Construct the labels\n",
    "        tmpMag = pd.read_csv(magFile)\n",
    "        tmpAng = pd.read_csv(angFile)\n",
    "        self.mags = torch.Tensor(tmpMag.to_numpy())\n",
    "        self.angs = torch.Tensor(tmpAng.to_numpy())\n",
    "        \n",
    "        # Construct the features and place them into readings array(X). \n",
    "        tmpReadings = pd.read_csv(readingsFile)\n",
    "        tmpReadings = tmpReadings.to_numpy()\n",
    "        print(tmpReadings.shape)\n",
    "        LL = tmpReadings.shape[0]\n",
    "        tmpReadings2 = np.zeros((LL, 3))\n",
    "        for k in range(LL):\n",
    "            tmpReadings2[k, 0] = np.max(np.abs(tmpReadings[k, :]))\n",
    "            tmpReadings2[k, 1] = np.argmax(np.abs(tmpReadings[k, :]))\n",
    "            tt = int(tmpReadings2[k,1])\n",
    "            tmpReadings2[k, 2] = np.abs(tmpReadings[k, (tt-1)%6])/(np.abs(tmpReadings[k, (tt+1)%6]) + 0.05)\n",
    "        \n",
    "        self.readings = torch.Tensor(tmpReadings2)\n",
    "        \n",
    "        # Incorporate the transforms as needed\n",
    "        self.transform=transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mags)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        reading = self.readings[idx, :]\n",
    "        mag = self.mags[idx]\n",
    "        ang = self.angs[idx]\n",
    "        label = torch.cat((mag, ang), 0)\n",
    "        \n",
    "        if self.transform:\n",
    "            reading = self.transform(reading)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return reading, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindMagDataset(Dataset):\n",
    "    '''\n",
    "    Dataset class for pytorch-based learning for gust magnitude data training. This method \n",
    "    learns directly from the sensor readings (voltages) to predict gust speed (m/s). \n",
    "    [Inputs]\n",
    "       --- The sensor readings (voltages)\n",
    "    [Predictions]\n",
    "       --- The gust speed (m/s)\n",
    "    '''\n",
    "    def __init__(self, magFile, readingsFile, transform=None, target_transform=None):\n",
    "        tmpMag = pd.read_csv(magFile)\n",
    "        tmpReadings = pd.read_csv(readingsFile)\n",
    "        self.mags = torch.Tensor(tmpMag.to_numpy())\n",
    "        self.readings = torch.Tensor(tmpReadings.to_numpy())\n",
    "        \n",
    "        # Incorporate the transforms as needed\n",
    "        self.transform=transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mags)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        reading = self.readings[idx, :]\n",
    "        label = self.mags[idx]\n",
    "        if self.transform:\n",
    "            reading = self.transform(reading)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return reading, label\n",
    "\n",
    "class WindAngDataset(Dataset):\n",
    "    '''\n",
    "    Dataset class for pytorch-based learning for gust angle data training. This method \n",
    "    learns directly from the sensor readings (voltages) to predict gust incidence angle (radians). \n",
    "    [Inputs]\n",
    "       --- The sensor readings (voltages)\n",
    "    [Predictions]\n",
    "       --- The gust angle (rad)\n",
    "    '''\n",
    "    def __init__(self, angFile, readingsFile, transform=None, target_transform=None):\n",
    "        tmpAng = pd.read_csv(angFile)\n",
    "        tmpReadings = pd.read_csv(readingsFile)\n",
    "        self.angs = torch.Tensor(tmpAng.to_numpy())\n",
    "        self.readings = torch.Tensor(tmpReadings.to_numpy())\n",
    "        \n",
    "        # Incorporate the transforms as needed\n",
    "        self.transform=transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.angs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        reading = self.readings[idx, :]\n",
    "        label = self.angs[idx]\n",
    "        if self.transform:\n",
    "            reading = self.transform(reading)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return reading, label\n",
    "\n",
    "class WindAngTrigDataset(Dataset):\n",
    "    '''\n",
    "    Dataset class for pytorch-based learning for gust angle data training. This method \n",
    "    learns directly from the sensor readings (voltages) to predict gust incidence angle (radians). \n",
    "    [Inputs]\n",
    "       --- The sensor readings (voltages)\n",
    "    [Predictions]\n",
    "       --- The gust angle (rad)\n",
    "    '''\n",
    "    def __init__(self, angFile, readingsFile, transform=None, target_transform=None):\n",
    "        tmpAng = pd.read_csv(angFile)\n",
    "        tmpReadings = pd.read_csv(readingsFile)\n",
    "        numpyAngs = tmpAng.to_numpy()\n",
    "        self.angs = torch.Tensor(np.array((np.cos(numpyAngs), np.sin(numpyAngs))))\n",
    "        self.readings = torch.Tensor(tmpReadings.to_numpy())\n",
    "        \n",
    "        # Incorporate the transforms as needed\n",
    "        self.transform=transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.angs[:, 0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        reading = self.readings[idx, :]\n",
    "        label = self.angs[idx, :]\n",
    "        if self.transform:\n",
    "            reading = self.transform(reading)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return reading, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    '''\n",
    "    A general/generic Neural Network model class for use with Pytorch. \n",
    "    \n",
    "    TODO: include layer widths, types, and nonlinearities as inputs and dynamically allocate\n",
    "          --- this will allow for custom classes rather than the clunky \"if\" statement used here. \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrosswireNet(NeuralNetwork):\n",
    "    '''\n",
    "    A general/generic Neural Network model class for use with Pytorch. \n",
    "    \n",
    "    TODO: include layer widths, types, and nonlinearities as inputs and dynamically allocate\n",
    "          --- this will allow for custom classes rather than the clunky \"if\" statement used here. \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(CrosswireNet, self).__init__()\n",
    "        \n",
    "        self.linear_relu_stack = nn.Sequential(nn.Linear(3, 25),\n",
    "                                               nn.ReLU(),\n",
    "                                               nn.Linear(25, 15),\n",
    "                                               nn.ReLU(),\n",
    "                                               nn.Linear(15, 2),\n",
    "                                               )\n",
    "    def forward(self, x):\n",
    "        # Method to propagate input (reading) through the network to get a prediction. \n",
    "        # Terminology is clunky because this is adapted from a classification example, hence \n",
    "        # the use of 'logits' even though we are doing regression.\n",
    "        \n",
    "        # TODO -- tidy up variable names, usage, etc (see above)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class AngleNet(NeuralNetwork):\n",
    "    '''\n",
    "    A general/generic Neural Network model class for use with Pytorch. \n",
    "    \n",
    "    TODO: include layer widths, types, and nonlinearities as inputs and dynamically allocate\n",
    "          --- this will allow for custom classes rather than the clunky \"if\" statement used here. \n",
    "    '''\n",
    "    def __init__(self, geom=6, trigValue=1):\n",
    "        super(CrosswireNet, self).__init__()\n",
    "        \n",
    "        k1 = int(geom*8)\n",
    "        k2 = int(k1/2 + 5)\n",
    "        self.linear_relu_stack = nn.Sequential(nn.Linear(geom, k1),\n",
    "                                               nn.ReLU(),\n",
    "                                               nn.Linear(k1, k2),\n",
    "                                               nn.ReLU(),\n",
    "                                               nn.Linear(k2, trigValue),\n",
    "                                               )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Method to propagate input (reading) through the network to get a prediction. \n",
    "        # Terminology is clunky because this is adapted from a classification example, hence \n",
    "        # the use of 'logits' even though we are doing regression.\n",
    "        \n",
    "        # TODO -- tidy up variable names, usage, etc (see above)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, optimizer, epochNum, seedNum, loss_fn=nn.L1Loss(), verbose=True, batch_size=180, writePath=None):\n",
    "    \"\"\"\n",
    "    Loop for training a 'model' (class NeuralNetwork) on data stored in 'dataloader,' using loss function\n",
    "    'loss_fn' and optimizer method 'optimizer'\n",
    "    \n",
    "    [Inputs]\n",
    "    dataloader    -- type DataLoader    -- Pytorch DataLoader object to facilitate training/testing data storage\n",
    "                                           to interface with pytorch optimization and training modules\n",
    "                                           \n",
    "    model         -- type NeuralNetwork -- Pytorch NeuralNetwork object to facilitate training/testing of speed and \n",
    "                                           angle prediction for FlowDrone\n",
    "                                           \n",
    "    epochNum      -- type int           -- Current epoch number to track training loss\n",
    "    \n",
    "    seedNum       -- type int           -- Seed of the current run (to average over to demonstrate convergence)\n",
    "    \n",
    "    loss_fn       -- type torch.nn loss -- Pytorch loss function (in nn library) for training the speed/angle predictor\n",
    "                                           for FlowDrone. Defaults to nn.MSELoss() because we are regressing real-valued\n",
    "                                           variables. \n",
    "                                           \n",
    "    optimizer     -- type torch.optim   -- Pytorch optimizer for ANN weight updates. Normally will use ADAM unless there\n",
    "                                           is a compelling reason to deviate. \n",
    "    \n",
    "    verbose       -- type Boolean       -- Toggles printing of training loss during training. Default is TRUE. \n",
    "    \n",
    "    writePath     -- type String        -- If present, a path to write to a csv file in order \n",
    "    [Outputs]\n",
    "    \n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    f = open(writePath+'trainCost'+str(seedNum)+'.csv', 'a')\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        '''\n",
    "        for k in range(pred.shape[0]):\n",
    "            if ((pred[k, -1] - y[k, -1]) >= math.pi):\n",
    "                while ((pred[k, -1] - y[k, -1]) >= math.pi):\n",
    "                    pred[k, -1] -= 2.0*math.pi\n",
    "            elif ((pred[k, -1] - y[k, -1]) <= -math.pi):\n",
    "                pred[k, -1] = (pred[k,-1])%(2.0*math.pi)\n",
    "        '''\n",
    "        loss = loss_fn.forward(y, pred)\n",
    "        loss_average = (loss/pred.shape[0]).cpu().detach().numpy()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 200 == 199 or (batch==0 and epochNum==1):\n",
    "            if verbose:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "            if writePath is None:\n",
    "                pass\n",
    "            else: \n",
    "                writer.writerow(np.array([(batch + (np.ceil(size/batch_size)*epochNum)), 180.0*loss_average/math.pi]))\n",
    "                \n",
    "\n",
    "    # close the file\n",
    "    f.close()\n",
    "        \n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, epochNum, seedNum, loss_fn=nn.L1Loss(), lastLoop=False, writePath=None):\n",
    "    \"\"\"\n",
    "    Loop for test a 'model' (class NeuralNetwork) on data stored in 'dataloader,' using loss function\n",
    "    'loss_fn.'\n",
    "    \n",
    "    [Inputs]\n",
    "    dataloader    -- type DataLoader    -- Pytorch DataLoader object to facilitate training/testing data storage\n",
    "                                           to interface with pytorch optimization and training modules\n",
    "                                           \n",
    "    model         -- type NeuralNetwork -- Pytorch NeuralNetwork object to facilitate training/testing of speed and \n",
    "                                           angle prediction for FlowDrone\n",
    "\n",
    "    epochNum      -- type Int           -- Current epoch\n",
    "    \n",
    "    loss_fn       -- type torch.nn loss -- Pytorch loss function (in nn library) for training the speed/angle predictor\n",
    "                                           for FlowDrone. Defaults to nn.MSELoss() because we are regressing real-valued\n",
    "                                           variables. \n",
    "                                           \n",
    "    lastLoop      -- type Boolean       -- Whether we are on the last epoch (for histogram information)\n",
    "    \n",
    "    writePath     -- type String        -- File to write to\n",
    "    [Outputs]\n",
    "    \n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    batch_size = dataloader.batch_size\n",
    "    hists = False\n",
    "    \n",
    "    if lastLoop:\n",
    "        if (num_batches == size):\n",
    "            print('On the last loop!')\n",
    "            errs = np.zeros((size, 2))\n",
    "            idxVal = 0\n",
    "            hists = True\n",
    "        \n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            '''\n",
    "            for k in range(pred.shape[0]):\n",
    "                if ((pred[k, -1] - y[k, -1]) >= math.pi):\n",
    "                    while ((pred[k, -1] - y[k, -1]) >= math.pi):\n",
    "                        pred[k, -1] -= 2.0*math.pi\n",
    "                elif ((pred[k, -1] - y[k, -1]) <= -math.pi):\n",
    "                    pred[k, -1] = (pred[k,-1])%(2.0*math.pi)\n",
    "            '''\n",
    "            if hists:\n",
    "                errs[idxVal, :] = np.array([y, loss_fn.forward(y, pred)])\n",
    "                # errs[idxVal, :] = np.array([y, loss_fn(y, pred).item()])\n",
    "                test_loss += errs[idxVal, 1]\n",
    "                idxVal += 1\n",
    "            else:\n",
    "                test_loss += loss_fn.forward(y, pred)/pred.shape[0]\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss (rad): {test_loss:>8f}\")\n",
    "    print(f\"Avg error (deg): {(test_loss*180.0/math.pi):>8f} \\n\")\n",
    "    \n",
    "    if writePath is None:\n",
    "        pass\n",
    "    else:\n",
    "        # open the file\n",
    "        f = open(writePath+'testCost'+str(seedNum)+'.csv', 'a')\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        # write out the relevant cost\n",
    "        writer.writerow(np.array([epochNum, test_loss*180.0/math.pi]))\n",
    "                \n",
    "        # close the file\n",
    "        f.close()\n",
    "    \n",
    "    if hists:\n",
    "        np.savetxt(writePath+'testErrs'+str(seedNum)+'.csv', errs, delimiter=',')\n",
    "        return test_loss, errs\n",
    "    else: \n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDataset(dataSetType=2, geometryVal=3, compFlag=True, compString=None, N=1, epochs0=15):\n",
    "    # Make dataset\n",
    "    # Change this as desired in {1, 2, 3, 4, 5}\n",
    "    #\n",
    "    # INDEX: \n",
    "    #   --- (1) Sparse wind magnitudes                         [OLD]\n",
    "    #   --- (2) Sparse wind angles (10-degree increments)      [OLD]\n",
    "    #   --- (3) Dense Crosswire Model \n",
    "    #   --- (4) Dense Wind Magnitudes \n",
    "    #   --- (5) Dense Incidence Angles (2-degree increments)\n",
    "    \n",
    "    ### dataSetType = 2\n",
    "\n",
    "    # 3=Triangle, 4=Square, 5=Pentagon, 6=Hexagon\n",
    "    ### geometryVal = 3\n",
    "    ### compFlag=True\n",
    "    ### N = 1                 # Number of sequentially averaged data points\n",
    "\n",
    "    if geometryVal == 3:\n",
    "        geomPath='tri/'\n",
    "    elif geometryVal == 4:\n",
    "        geomPath='squ/'\n",
    "    elif geometryVal == 5:\n",
    "        geomPath='pent/'\n",
    "    elif geometryVal == 6:\n",
    "        geomPath='hex/'\n",
    "    else:\n",
    "        raise ValueError('Geometry must be in {3, 4, 5, 6}')\n",
    "\n",
    "    testPathBase = 'compVal' if compString is None else compString\n",
    "    \n",
    "    trainLabelPath = 'compTrain/'\n",
    "    testLabelPath = testPathBase+'/'\n",
    "    trainPath='compTrain_N'+str(N)+'/'\n",
    "    testPath= testPathBase+'_N'+str(N)+'/'       # Set to validation data for network/hyperparameter optimization, else test data\n",
    "\n",
    "    # Don't change these; the 'if' statements take care of them\n",
    "    # Set network parameters in NeuralNetwork class\n",
    "    fullAnglesVal = False\n",
    "    crosswireVal = False\n",
    "\n",
    "    if dataSetType==1:\n",
    "        if compFlag:\n",
    "            trainY = trainLabelPath+'mags.csv'\n",
    "            trainX = trainPath+geomPath+'readings.csv'\n",
    "            testY = testLabelPath+'mags.csv'\n",
    "            testX = testPath+geomPath+'readings.csv'\n",
    "        else:\n",
    "            trainY = 'MagTrain/mags.csv'\n",
    "            trainX = 'MagTrain/readings.csv'\n",
    "            testY = 'MagTest/mags.csv'\n",
    "            testX = 'MagTest/readings.csv'\n",
    "\n",
    "        training_data = WindMagDataset(trainY, trainX, transform=None)\n",
    "        testing_data = WindMagDataset(testY, testX, transform=None)\n",
    "        epochs = epochs0\n",
    "\n",
    "    elif dataSetType==2:\n",
    "        if compFlag:\n",
    "            trainY = trainLabelPath+'angsrad.csv'\n",
    "            trainX = trainPath+geomPath+'readings.csv'\n",
    "            testY = testLabelPath+'angsrad.csv'\n",
    "            testX = testPath+geomPath+'readings.csv'\n",
    "            fullAnglesVal = True\n",
    "\n",
    "        else:\n",
    "            trainY = 'MagTrain/angsrad.csv'\n",
    "            trainX = 'MagTrain/readings.csv'\n",
    "            testY = 'MagTest/angsrad.csv'\n",
    "            testX = 'MagTest/readings.csv'\n",
    "\n",
    "        training_data = WindAngDataset(trainY, trainX, transform=None)\n",
    "        testing_data = WindAngDataset(testY, testX, transform=None)\n",
    "        epochs = epochs0\n",
    "\n",
    "    elif dataSetType==3:\n",
    "        trainY1 = 'CrossTrain/crossmags.csv'\n",
    "        trainY2 = 'CrossTrain/crossangsrad.csv'\n",
    "        trainX = 'CrossTrain/crossreadings.csv'\n",
    "        testY1 = 'CrossTest/crossmags.csv'\n",
    "        testY2 = 'CrossTest/crossangsrad.csv'\n",
    "        testX = 'CrossTest/crossreadings.csv'\n",
    "\n",
    "        training_data = CrossWireDataset(trainY1, trainY2, trainX, transform=None)\n",
    "        testing_data = CrossWireDataset(testY1, testY2, testX, transform=None)\n",
    "        epochs = 2*epochs0\n",
    "\n",
    "        crosswireVal = True\n",
    "\n",
    "    elif dataSetType==4:\n",
    "\n",
    "        trainY = 'CrossTrain/crossmags.csv'\n",
    "        trainX = 'CrossTrain/crossreadings.csv'\n",
    "        testY = 'CrossTest/crossmags.csv'\n",
    "        testX = 'CrossTest/crossreadings.csv'\n",
    "\n",
    "        training_data = WindMagDataset(trainY, trainX, transform=None)\n",
    "        testing_data = WindMagDataset(testY, testX, transform=None)\n",
    "        epochs = epochs0\n",
    "\n",
    "    elif dataSetType==5:\n",
    "        trainY = 'CrossTrain/crossangsrad.csv'\n",
    "        trainX = 'CrossTrain/crossreadings.csv'\n",
    "        testY = 'CrossTest/crossangsrad.csv'\n",
    "        testX = 'CrossTest/crossreadings.csv'\n",
    "\n",
    "        training_data = WindAngDataset(trainY, trainX, transform=None)\n",
    "        testing_data = WindAngDataset(testY, testX, transform=None)\n",
    "        epochs = epochs0\n",
    "\n",
    "        fullAnglesVal = True\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Not a valid dataSetType index (must be in {1, 2, 3, 4, or 5})')\n",
    "\n",
    "    '''\n",
    "    # Make training and testing data\n",
    "    train_dataloader = DataLoader(training_data, batch_size=180, shuffle=True)\n",
    "    test_dataloader = DataLoader(testing_data, batch_size=72, shuffle=True)\n",
    "    model = NeuralNetwork(crosswire=crosswireVal, fullAngles=fullAnglesVal, geom=geometryVal)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    '''\n",
    "    \n",
    "    return training_data, testing_data, epochs, fullAnglesVal, crosswireVal, trainPath, trainLabelPath, testPath, testLabelPath, geomPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLoss():\n",
    "    def __init__(self):\n",
    "        self.pi = math.pi\n",
    "        \n",
    "    def forward(self, y, yhat):\n",
    "        self.Errs = torch.cat((torch.remainder(y-yhat, 2.0*self.pi), torch.sub(torch.remainder(y-yhat, 2.0*self.pi), 2.0*self.pi)), 1)\n",
    "        tmp1 = torch.min(torch.abs(self.Errs), 1).values\n",
    "        return torch.sum(tmp1)\n",
    "        # return torch.sum(torch.min(torch.abs(torch.tensor([self.Errs, self.Errs-2.0*math.pi])), 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define needed learning quantities\n",
    "\n",
    "# Learning rate (initial)\n",
    "# ### Generally ~ 1e-3 for Adam\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Batch size (default 64)\n",
    "batch_size = 180\n",
    "\n",
    "# Number of training epochs for \n",
    "# ### the simpler regression problems\n",
    "epochs0 = 20\n",
    "\n",
    "# Loss Function (MSE/MAE usually because we are \n",
    "# running relatively standard regression)\n",
    "\n",
    "### ### Mean Squared error loss\n",
    "# loss_fn = nn.MSELoss()\n",
    "\n",
    "# Mean Absolute Error Loss\n",
    "# loss_fn = nn.L1Loss()\n",
    "\n",
    "# Mean Absolute Error Loss **wrapped on angles**\n",
    "loss_fn = customLoss()\n",
    "\n",
    "# Verbose flag toggles training\n",
    "verboseFlag=False\n",
    "\n",
    "# training_data, testing_data, epochs, fullAnglesVal, crosswireVal, trainPath, testPath = makeDataset(dataSetType=2, geometryVal=3, compFlag=True, N=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NN = np.array([1, 2, 5])\n",
    "GEOMVAL = np.arange(5, 7)\n",
    "\n",
    "nFilt = len(NN)\n",
    "nGeom = len(GEOMVAL)\n",
    "nSeed = 5\n",
    "tic()\n",
    "for jj in range(len(GEOMVAL)):\n",
    "    for ii in range(len(NN)):\n",
    "        bestPerformance = 100.0\n",
    "        N = NN[ii]\n",
    "        geometryVal = GEOMVAL[jj]\n",
    "        for kk in range(nSeed):\n",
    "            np.random.seed(kk*12345 + 31415*N*(geometryVal**3))\n",
    "            # Make all necessary data\n",
    "            training_data, testing_data, epochs, fullAnglesVal, crosswireVal, trainPath, trainLabelPath, testPath, testLabelPath, geomPath = makeDataset(dataSetType=2, geometryVal=geometryVal, compFlag=True, compString='compTest', N=N, epochs0=epochs0)\n",
    "            \n",
    "            # Make training and testing dataloaders\n",
    "            # Initialize model and optimizer params\n",
    "            train_dataloader = DataLoader(training_data, batch_size=180, shuffle=True)\n",
    "            test_dataloader = DataLoader(testing_data, batch_size=72, shuffle=True)\n",
    "            test_dataloader2 = DataLoader(testing_data, batch_size=1, shuffle=True)\n",
    "            model = AngleNet(geom=geometryVal)\n",
    "            opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            \n",
    "            print(f\"Epoch {0}\\n-------------------------------\")\n",
    "            avg_error = test_loop(test_dataloader, model, 0, kk, loss_fn, lastLoop=False, writePath=(testPath+geomPath))\n",
    "            \n",
    "            \n",
    "            for t in range(1, epochs+1):\n",
    "                print(f\"Epoch {t}\\n-------------------------------\")\n",
    "                train_loop(train_dataloader, model, opt, t, kk, loss_fn, verbose=verboseFlag, writePath=(testPath+geomPath))\n",
    "                print()\n",
    "                # if (float(t+1)/float(epochs) >= k or (t==(epochs-1))):\n",
    "                if t < epochs:\n",
    "                    avg_error = test_loop(test_dataloader, model, t, kk, loss_fn, lastLoop=False, writePath=(testPath+geomPath))\n",
    "                else: \n",
    "                    # avg_error, Z = test_loop(test_dataloader, model, loss_fn, lastLoop=(t==(epochs-1)))\n",
    "                    avg_error, Z = test_loop(test_dataloader2, model, t, kk, loss_fn, lastLoop=True, writePath=(testPath+geomPath))\n",
    "            \n",
    "            if avg_error < bestPerformance:\n",
    "                bestPerformance = avg_error\n",
    "                PATH = 'SavedModels/N'+str(N)+'_G'+str(geometryVal)+'_best.tar'\n",
    "                torch.save({\n",
    "                        'epoch': epochs,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': opt.state_dict(),\n",
    "                        'loss': avg_error*180.0/math.pi,\n",
    "                        }, PATH)\n",
    "            # Print that we have finished training\n",
    "            print(f\"Finished seed: {kk+1:>2d} of \"+str(nSeed)+f\", on geometry {jj+1:>2d} of \"+str(len(GEOMVAL))+f\", on filtering setting {ii+1:>2d} of \"+str(len(NN))+\" \\n\")\n",
    "\n",
    "            \n",
    "# Output time elapsed in seconds            \n",
    "dT = toc()\n",
    "dT = np.round_(dT)\n",
    "dT2 = dT % 3600\n",
    "dT3 = dT2 % 60\n",
    "print(f\"Elapsed time is {dT} seconds\")\n",
    "print(f\"This is equivalent to {dT // 3600} hours, \"+f\"{dT2 // 60} minutes, and \"+f\"{dT3} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triangle -- Best: ~20.6 degrees mean error; ~30 epochs; network layers 3-30-15-1; \n",
    "#          -- tested +10 more epochs and stalled in 20.9-21.5 test error range\n",
    "#\n",
    "# Square   -- ~4.5 degrees mean error; ~330 epochs; network layers [4]-50-25-1\n",
    "#          -- tested +00 more epochs, stalled in 6-6.5 range\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes for saving and loading Pytorch Models, from \n",
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "'''\n",
    "model = TheModelClass(*args, **kwargs)\n",
    "optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "# - or -\n",
    "model.train()\n",
    "'''\n",
    "# Useful reference for saving off weights, etc, that we might need and/or want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data processing section \n",
    "# Order in bigData is [tri -- squ -- pent -- hex]\n",
    "processData = False\n",
    "\n",
    "# Safety mechanism to not override data\n",
    "if processData:\n",
    "    testPathBase='compTest'\n",
    "    NN = np.array([1, 2, 5])\n",
    "    lenTraj = 21\n",
    "    nFilt = 3\n",
    "    nGeom = 4\n",
    "    nSeed = 5\n",
    "    BIGDAT = np.zeros((nFilt*nGeom*nSeed*lenTraj, 5))\n",
    "    for ii in range(nFilt):\n",
    "        for jj in np.arange(3, 3+nGeom):\n",
    "            geometryVal = jj\n",
    "            for kk in range(nSeed):\n",
    "                if geometryVal == 3:\n",
    "                    geomPath='tri/'\n",
    "                    geoString = 'TRI'\n",
    "                elif geometryVal == 4:\n",
    "                    geomPath='squ/'\n",
    "                    geoString = 'SQU'\n",
    "                elif geometryVal == 5:\n",
    "                    geomPath='pent/'\n",
    "                    geoString = 'PENT'\n",
    "                elif geometryVal == 6:\n",
    "                    geomPath='hex/'\n",
    "                    geoString = 'HEX'\n",
    "                else:\n",
    "                    raise ValueError('Geometry must be in {3, 4, 5, 6}')\n",
    "\n",
    "                testPath=testPathBase+'_N'+str(NN[ii])+'/' + geomPath + 'testCost'+str(kk)+'.csv'\n",
    "                # testPathInit = 'compVal_N'+str(NN[ii])+'/' + geomPath + 'testCostInit'+str(kk)+'.csv'\n",
    "\n",
    "                tmp = np.zeros((lenTraj, 5))\n",
    "                # tmp[0,:2] = pd.read_csv(testPathInit, header=None).to_numpy()\n",
    "                tmp[0:,:2] = pd.read_csv(testPath, header=None).to_numpy()\n",
    "                # tmp[1:,0] += 1\n",
    "                tmp[:,2] = np.ones(lenTraj)*(kk+1)\n",
    "                tmp[:,3] = np.ones(lenTraj)*geometryVal\n",
    "                tmp[:,4] = np.ones(lenTraj)*NN[ii]\n",
    "\n",
    "                BIGDAT[(ii*nGeom*nSeed*lenTraj + (jj-3)*nSeed*lenTraj + kk*lenTraj):(ii*nGeom*nSeed*lenTraj + (jj-3)*nSeed*lenTraj + (kk+1)*lenTraj), :] = tmp\n",
    "\n",
    "    # Save to npy file\n",
    "    np.save('SummaryStats/BigData.npy', BIGDAT)\n",
    "else: \n",
    "    raise ValueError('Did not process the data due to the processData flag being False. Make sure you want to process the data before proceeding!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process testErrs into a dataframe and condense to averages\n",
    "# Order in bigData is [tri -- squ -- pent -- hex]\n",
    "\n",
    "# Safety mechanism to not override data\n",
    "if processData:\n",
    "    \n",
    "    testPathBase='compTest'\n",
    "    NN = np.array([1, 2, 5])\n",
    "    lenDat = 287999\n",
    "    nFilt = len(NN)\n",
    "    nGeom = 4\n",
    "    nSeed = 5\n",
    "    dfMain = pd.DataFrame(columns=('Angle_deg', 'Err_deg', 'N', 'Geometry'))\n",
    "    dfMain2 = pd.DataFrame(columns=('Angle_deg', 'Err_deg', 'N', 'Geometry'))\n",
    "    for ii in range(nFilt):\n",
    "        for jj in np.arange(3, 3+nGeom):\n",
    "            geometryVal = jj\n",
    "            tmpDAT = np.zeros((nSeed*lenDat, 5))\n",
    "            bestSeed = 900.0*np.ones(nSeed)\n",
    "            for kk in range(nSeed):\n",
    "                if geometryVal == 3:\n",
    "                    geomPath='tri/'\n",
    "                    geoString = 'TRI'\n",
    "                elif geometryVal == 4:\n",
    "                    geomPath='squ/'\n",
    "                    geoString = 'SQU'\n",
    "                elif geometryVal == 5:\n",
    "                    geomPath='pent/'\n",
    "                    geoString = 'PENT'\n",
    "                elif geometryVal == 6:\n",
    "                    geomPath='hex/'\n",
    "                    geoString = 'HEX'\n",
    "                else:\n",
    "                    raise ValueError('Geometry must be in {3, 4, 5, 6}')\n",
    "\n",
    "                trainPath = testPathBase+'_N'+str(NN[ii])+'/' + geomPath + 'trainCost'+str(kk)+'.csv' \n",
    "                testPath = testPathBase+'_N'+str(NN[ii])+'/' + geomPath + 'testErrs'+str(kk)+'.csv'\n",
    "                # testPathInit = 'compVal_N'+str(NN[ii])+'/' + geomPath + 'testCostInit'+str(kk)+'.csv'\n",
    "\n",
    "                tmp0 = pd.read_csv(trainPath, header=None).to_numpy()\n",
    "                bestSeed[kk] = tmp0[-1, 1]\n",
    "                # print(tmp0[-1, 1])\n",
    "\n",
    "                tmp = np.zeros((lenDat, 5))\n",
    "                # tmp[0,:2] = pd.read_csv(testPathInit, header=None).to_numpy()\n",
    "                tmp[:,:2] = pd.read_csv(testPath, header=None).to_numpy()\n",
    "                tmp[:,:2] *= 180.0/math.pi\n",
    "                tmp[:,0] = np.round_(tmp[:,0])\n",
    "                # bestSeed[kk] = np.mean(tmp[:,1])\n",
    "                tmp[:,2] = np.ones(lenDat)*NN[ii]\n",
    "                tmp[:,3] = np.ones(lenDat)*geometryVal\n",
    "                tmp[:,4] = np.ones(lenDat)*(kk+1)\n",
    "                tmpDAT[kk*lenDat:(kk+1)*lenDat,:] = tmp\n",
    "\n",
    "\n",
    "            # Choose the best seed for each geometry-filter configuration\n",
    "            kkStar = np.argmin(bestSeed)\n",
    "            # print(bestSeed)\n",
    "            # print(f\"The best seed is Seed {kkStar+1}, representing element\"+ f\" [{kkStar}] of bestSeed array\")\n",
    "            # breakpoint()\n",
    "\n",
    "            dfTmp = pd.DataFrame(data=tmpDAT[kkStar*lenDat:(kkStar+1)*lenDat,:4],\n",
    "                                 columns=('Angle_deg', 'Err_deg', 'N', 'Geometry')\n",
    "                                )\n",
    "\n",
    "            dfMain = pd.concat([dfMain, dfTmp])\n",
    "\n",
    "            dfTmp2 = dfTmp.groupby(['Angle_deg'], as_index=False).mean()\n",
    "            # print(dfTmp2.shape)\n",
    "            # Append to main dataframe\n",
    "            dfMain2 = pd.concat([dfMain2, dfTmp2])\n",
    "            # print(dfMain.shape)\n",
    "\n",
    "\n",
    "            # BIGDAT[(ii*nGeom*nSeed*lenTraj + (jj-3)*nSeed*lenTraj + kk*lenTraj):(ii*nGeom*nSeed*lenTraj + (jj-3)*nSeed*lenTraj + (kk+1)*lenTraj), :] = tmp\n",
    "\n",
    "    # Save to csv file\n",
    "    dfMain.to_csv('SummaryStats/ErrorVSAngleLarge.csv')\n",
    "    dfMain2.to_csv('SummaryStats/ErrorVSAngle.csv')\n",
    "\n",
    "else: \n",
    "    raise ValueError('Did not process the data due to the processData flag being False. Make sure you want to process the data before proceeding!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zs = np.sort(Z)\n",
    "np.savetxt(testPath+geomPath+'ValErrorSort.csv', Zs, delimiter=',')\n",
    "\n",
    "print('Mean Absolute Error (deg): ', str(np.mean(Zs)*180.0/math.pi))\n",
    "\n",
    "# Choose what fraction of Zs to study\n",
    "fracPred = 0.98\n",
    "\n",
    "\n",
    "# Take the >fracPred set of best predictions\n",
    "nKeep = int(np.ceil(fracPred*len(Zs)))\n",
    "Zsmall = Zs[:nKeep]\n",
    "\n",
    "# Print the fracPred quantile worst prediction\n",
    "print(np.max(Zsmall))\n",
    "print('This is equivalent to '+ str(np.max(Zsmall)*180.0/math.pi) + ' degrees')\n",
    "print('Mean Absolute Error, best 98% (deg): ', str(np.mean(Zsmall)*180.0/math.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "plt.hist(Zsmall, cumulative=False, density=True, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = int(len(Zsmall)*0.84)\n",
    "print(zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Zsmall.shape)\n",
    "print(Zsmall[zs]*180/math.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = np.array([1, 2, 5])\n",
    "print(NN[0])\n",
    "print(NN.shape)\n",
    "GEOMVAL = np.arange(3, 7)\n",
    "print(GEOMVAL)\n",
    "print(GEOMVAL.shape)\n",
    "# training_data, testing_data, epochs, fullAnglesVal, crosswireVal, trainPath, testPath = makeDataset(dataSetType=2, geometryVal=3, compFlag=True, N=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "# Make dataset\n",
    "# Change this as desired in {1, 2, 3, 4, 5}\n",
    "#\n",
    "# INDEX: \n",
    "#   --- (1) Sparse wind magnitudes                         [OLD]\n",
    "#   --- (2) Sparse wind angles (10-degree increments)      [OLD]\n",
    "#   --- (3) Dense Crosswire Model \n",
    "#   --- (4) Dense Wind Magnitudes \n",
    "#   --- (5) Dense Incidence Angles (2-degree increments)\n",
    "dataSetType = 2\n",
    "\n",
    "# 3=Triangle, 4=Square, 5=Pentagon, 6=Hexagon\n",
    "geometryVal = 3\n",
    "compFlag=True\n",
    "N = 1                 # Number of sequentially averaged data points\n",
    "\n",
    "if geometryVal == 3:\n",
    "    geomPath='tri/'\n",
    "elif geometryVal == 4:\n",
    "    geomPath='squ/'\n",
    "elif geometryVal == 5:\n",
    "    geomPath='pent/'\n",
    "elif geometryVal == 6:\n",
    "    geomPath='hex/'\n",
    "else:\n",
    "    raise ValueError('Geometry must be in {3, 4, 5, 6}')\n",
    "\n",
    "trainPath='compTrain_N'+str(N)+'/'\n",
    "testPath='compVal_N'+str(N)+'/'       # Set to validation data for network/hyperparameter optimization, else test data\n",
    "\n",
    "# Don't change these; the 'if' statements take care of them\n",
    "# Set network parameters in NeuralNetwork class\n",
    "fullAnglesVal = False\n",
    "crosswireVal = False\n",
    "\n",
    "if dataSetType==1:\n",
    "    if compFlag:\n",
    "        trainY = trainPath+geomPath+'mags.csv'\n",
    "        trainX = trainPath+geomPath+'readings.csv'\n",
    "        testY = testPath+geomPath+'mags.csv'\n",
    "        testX = testPath+geomPath+'readings.csv'\n",
    "    else:\n",
    "        trainY = 'MagTrain/mags.csv'\n",
    "        trainX = 'MagTrain/readings.csv'\n",
    "        testY = 'MagTest/mags.csv'\n",
    "        testX = 'MagTest/readings.csv'\n",
    "    \n",
    "    training_data = WindMagDataset(trainY, trainX, transform=None)\n",
    "    testing_data = WindMagDataset(testY, testX, transform=None)\n",
    "    epochs = epochs0\n",
    "    \n",
    "elif dataSetType==2:\n",
    "    if compFlag:\n",
    "        trainY = trainPath+geomPath+'angsrad.csv'\n",
    "        trainX = trainPath+geomPath+'readings.csv'\n",
    "        testY = testPath+geomPath+'angsrad.csv'\n",
    "        testX = testPath+geomPath+'readings.csv'\n",
    "        fullAnglesVal = True\n",
    "        \n",
    "    else:\n",
    "        trainY = 'MagTrain/angsrad.csv'\n",
    "        trainX = 'MagTrain/readings.csv'\n",
    "        testY = 'MagTest/angsrad.csv'\n",
    "        testX = 'MagTest/readings.csv'\n",
    "    \n",
    "    training_data = WindAngDataset(trainY, trainX, transform=None)\n",
    "    testing_data = WindAngDataset(testY, testX, transform=None)\n",
    "    epochs = epochs0\n",
    "    \n",
    "elif dataSetType==3:\n",
    "    trainY1 = 'CrossTrain/crossmags.csv'\n",
    "    trainY2 = 'CrossTrain/crossangsrad.csv'\n",
    "    trainX = 'CrossTrain/crossreadings.csv'\n",
    "    testY1 = 'CrossTest/crossmags.csv'\n",
    "    testY2 = 'CrossTest/crossangsrad.csv'\n",
    "    testX = 'CrossTest/crossreadings.csv'\n",
    "    \n",
    "    training_data = CrossWireDataset(trainY1, trainY2, trainX, transform=None)\n",
    "    testing_data = CrossWireDataset(testY1, testY2, testX, transform=None)\n",
    "    epochs = 2*epochs0\n",
    "    \n",
    "    crosswireVal = True\n",
    "    \n",
    "elif dataSetType==4:\n",
    "    \n",
    "    trainY = 'CrossTrain/crossmags.csv'\n",
    "    trainX = 'CrossTrain/crossreadings.csv'\n",
    "    testY = 'CrossTest/crossmags.csv'\n",
    "    testX = 'CrossTest/crossreadings.csv'\n",
    "    \n",
    "    training_data = WindMagDataset(trainY, trainX, transform=None)\n",
    "    testing_data = WindMagDataset(testY, testX, transform=None)\n",
    "    epochs = epochs0\n",
    "    \n",
    "elif dataSetType==5:\n",
    "    trainY = 'CrossTrain/crossangsrad.csv'\n",
    "    trainX = 'CrossTrain/crossreadings.csv'\n",
    "    testY = 'CrossTest/crossangsrad.csv'\n",
    "    testX = 'CrossTest/crossreadings.csv'\n",
    "    \n",
    "    training_data = WindAngDataset(trainY, trainX, transform=None)\n",
    "    testing_data = WindAngDataset(testY, testX, transform=None)\n",
    "    epochs = epochs0\n",
    "    \n",
    "    fullAnglesVal = True\n",
    "    \n",
    "else:\n",
    "    raise ValueError('Not a valid dataSetType index (must be in {1, 2, 3, 4, or 5})')\n",
    "   \n",
    "'''\n",
    "'''\n",
    "# Make training and testing data\n",
    "train_dataloader = DataLoader(training_data, batch_size=180, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=72, shuffle=True)\n",
    "model = NeuralNetwork(crosswire=crosswireVal, fullAngles=fullAnglesVal, geom=geometryVal)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
